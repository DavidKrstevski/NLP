{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a61fbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yelp:                                                sentence  label\n",
      "0                             Wow... Loved this place.      1\n",
      "1                                   Crust is not good.      0\n",
      "2            Not tasty and the texture was just nasty.      0\n",
      "3    Stopped by during the late May bank holiday of...      1\n",
      "4    The selection on the menu was great and so wer...      1\n",
      "..                                                 ...    ...\n",
      "995  I think food should have flavor and texture an...      0\n",
      "996                           Appetite instantly gone.      0\n",
      "997  Overall I was not impressed and would not go b...      0\n",
      "998  The whole experience was underwhelming, and I ...      0\n",
      "999  Then, as if I hadn't wasted enough of my life ...      0\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "Amazon:                                                sentence  label\n",
      "0    So there is no way for me to plug it in here i...      0\n",
      "1                          Good case, Excellent value.      1\n",
      "2                               Great for the jawbone.      1\n",
      "3    Tied to charger for conversations lasting more...      0\n",
      "4                                    The mic is great.      1\n",
      "..                                                 ...    ...\n",
      "995  The screen does get smudged easily because it ...      0\n",
      "996  What a piece of junk.. I lose more calls on th...      0\n",
      "997                       Item Does Not Match Picture.      0\n",
      "998  The only thing that disappoint me is the infra...      0\n",
      "999  You can not answer calls with the unit, never ...      0\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "IMDB:                                                sentence  label\n",
      "0    A very, very, very slow-moving, aimless movie ...      0\n",
      "1    Not sure who was more lost - the flat characte...      0\n",
      "2    Attempting artiness with black & white and cle...      0\n",
      "3         Very little music or anything to speak of.        0\n",
      "4    The best scene in the movie was when Gerardo i...      1\n",
      "..                                                 ...    ...\n",
      "743  I just got bored watching Jessice Lange take h...      0\n",
      "744  Unfortunately, any virtue in this film's produ...      0\n",
      "745                   In a word, it is embarrassing.        0\n",
      "746                               Exceptionally bad!        0\n",
      "747  All in all its an insult to one's intelligence...      0\n",
      "\n",
      "[748 rows x 2 columns]\n",
      "All:                                                sentence  label\n",
      "0                             Wow... Loved this place.      1\n",
      "1                                   Crust is not good.      0\n",
      "2            Not tasty and the texture was just nasty.      0\n",
      "3    Stopped by during the late May bank holiday of...      1\n",
      "4    The selection on the menu was great and so wer...      1\n",
      "..                                                 ...    ...\n",
      "743  I just got bored watching Jessice Lange take h...      0\n",
      "744  Unfortunately, any virtue in this film's produ...      0\n",
      "745                   In a word, it is embarrassing.        0\n",
      "746                               Exceptionally bad!        0\n",
      "747  All in all its an insult to one's intelligence...      0\n",
      "\n",
      "[2748 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Load Data\n",
    "import pandas as pd\n",
    "\n",
    "df_yelp = pd.read_csv('./sentiment labelled sentences/yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_amazon = pd.read_csv('./sentiment labelled sentences/amazon_cells_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_imdb = pd.read_csv('./sentiment labelled sentences/imdb_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "\n",
    "print(\"Yelp: \", df_yelp)\n",
    "print(\"Amazon: \", df_amazon)\n",
    "print(\"IMDB: \", df_imdb)\n",
    "\n",
    "df_all = pd.concat([df_yelp, df_amazon, df_imdb])\n",
    "print(\"All: \", df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7e3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2748, 2)\n",
      "Missing: {'sentence': 0, 'label': 0}\n",
      "\n",
      "Label-Verteilung (Count/%)\n",
      "       count  percent\n",
      "label                \n",
      "0       1362    49.56\n",
      "1       1386    50.44\n",
      "\n",
      "Gesamt-Deskriptoren:\n",
      "               count    mean      std  min     25%     50%     75%     max\n",
      "label         2748.0   0.504    0.500  0.0   0.000   1.000   1.000     1.0\n",
      "char_len      2748.0  71.528  201.987  7.0  32.000  55.000  87.000  7944.0\n",
      "word_len      2748.0  12.896   33.489  0.0   6.000  10.000  16.000  1297.0\n",
      "avg_word_len  2748.0   4.498    0.974  0.0   3.923   4.364   4.909    13.0\n",
      "stop_ratio    2748.0   0.494    0.181  0.0   0.417   0.500   0.609     1.0\n",
      "ttr           2748.0   0.956    0.073  0.0   0.917   1.000   1.000     1.0\n",
      "\n",
      "Mittelwerte je Label:\n",
      "       char_len  word_len  avg_word_len  stop_ratio    ttr\n",
      "label                                                     \n",
      "0        74.302    13.467         4.462       0.515  0.955\n",
      "1        68.802    12.334         4.533       0.474  0.956\n",
      "\n",
      "Top Unigrams (gesamt):\n",
      "  1-gram  count\n",
      "0   good    231\n",
      "1  great    210\n",
      "2  movie    181\n",
      "3  phone    165\n",
      "4   film    160\n",
      "5   food    126\n",
      "6   like    125\n",
      "7   just    119\n",
      "8  place    114\n",
      "9   it's    114\n",
      "\n",
      "Top Bigrams (gesamt):\n",
      "             2-gram  count\n",
      "0        waste time     17\n",
      "1       works great     17\n",
      "2  customer service     15\n",
      "3     sound quality     14\n",
      "4       waste money     12\n",
      "5       don't waste     11\n",
      "6  highly recommend     11\n",
      "7      battery life     11\n",
      "8       don't think     10\n",
      "9         i've seen     10\n",
      "\n",
      "Top Unigrams (Label=0):\n",
      "  1-gram  count\n",
      "0    bad     96\n",
      "1  movie     95\n",
      "2  phone     78\n",
      "3   film     71\n",
      "4   just     69\n",
      "5   like     67\n",
      "6   food     66\n",
      "7   time     62\n",
      "8  don't     60\n",
      "9   good     57\n",
      "\n",
      "Top Bigrams (Label=0):\n",
      "             2-gram  count\n",
      "0        waste time     16\n",
      "1  customer service     12\n",
      "2       waste money     12\n",
      "3       don't waste     11\n",
      "4       don't think      8\n",
      "5         i've seen      8\n",
      "6         it's just      6\n",
      "7      doesn't work      6\n",
      "8         does work      6\n",
      "9         don't buy      6\n",
      "\n",
      "Top Unigrams (Label=1):\n",
      "   1-gram  count\n",
      "0   great    193\n",
      "1    good    174\n",
      "2    film     89\n",
      "3   phone     87\n",
      "4   movie     86\n",
      "5    food     60\n",
      "6  really     60\n",
      "7    it's     60\n",
      "8    best     59\n",
      "9   place     58\n",
      "\n",
      "Top Bigrams (Label=1):\n",
      "             2-gram  count\n",
      "0       works great     17\n",
      "1  highly recommend     11\n",
      "2     sound quality     10\n",
      "3       great phone      9\n",
      "4        great food      8\n",
      "5       really good      8\n",
      "6     great service      7\n",
      "7       pretty good      7\n",
      "8         food good      7\n",
      "9      good quality      7\n",
      "\n",
      "Vokabular gesamt (ohne Stopwords): 4903\n",
      "Vokabular je Label (ohne Stopwords):\n",
      "label\n",
      "0    3111\n",
      "1    3079\n",
      "Name: sentence, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Explore Data\n",
    "import re, numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "percent = df_yelp['label'].value_counts(normalize=True, dropna=False).sort_index() * 100\n",
    "print(\"Yelp Verteilung (%):\\n\", percent.round(2))\n",
    "\n",
    "percent = df_amazon['label'].value_counts(normalize=True, dropna=False).sort_index() * 100\n",
    "print(\"Amazon Verteilung (%):\\n\", percent.round(2))\n",
    "\n",
    "percent = df_imdb['label'].value_counts(normalize=True, dropna=False).sort_index() * 100\n",
    "print(\"IMDB Verteilung (%):\\n\", percent.round(2))\n",
    "\n",
    "df = df_all.copy()\n",
    "\n",
    "#Tokenizer\n",
    "_token_re = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
    "def tok(s): \n",
    "    if not isinstance(s, str): return []\n",
    "    return _token_re.findall(s.lower())\n",
    "\n",
    "#Grund√ºberblick\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Missing:\", df.isna().sum().to_dict())\n",
    "print(\"\\nLabel-Verteilung (Count/%)\")\n",
    "counts = df['label'].value_counts().sort_index()\n",
    "pct = (df['label'].value_counts(normalize=True).sort_index()*100).round(2)\n",
    "print(pd.DataFrame({'count': counts, 'percent': pct}))\n",
    "\n",
    "#Kern-Metriken je Text\n",
    "def stats(text):\n",
    "    s = text if isinstance(text, str) else \"\"\n",
    "    t = tok(s)\n",
    "    n = len(t)\n",
    "    return pd.Series({\n",
    "        'char_len': len(s),\n",
    "        'word_len': n,\n",
    "        'avg_word_len': (sum(len(x) for x in t)/n) if n else 0.0,\n",
    "        'stop_ratio': (sum(x in ENGLISH_STOP_WORDS for x in t)/(n or 1)),\n",
    "        'ttr': (len(set(t))/(n or 1)),\n",
    "    })\n",
    "\n",
    "fe = df['sentence'].apply(stats)\n",
    "df_e = pd.concat([df[['label']], fe], axis=1)\n",
    "\n",
    "print(\"\\nGesamt-Deskriptoren:\")\n",
    "print(df_e.describe().round(3).T)\n",
    "\n",
    "print(\"\\nMittelwerte je Label:\")\n",
    "print(df_e.groupby('label').mean(numeric_only=True).round(3))\n",
    "\n",
    "#N-Grams (Top 10)\n",
    "def top_ngrams(texts, n=1, k=10, drop_stop=True):\n",
    "    c = Counter()\n",
    "    for s in texts:\n",
    "        t = tok(s)\n",
    "        if drop_stop: t = [w for w in t if w not in ENGLISH_STOP_WORDS]\n",
    "        grams = t if n==1 else [\" \".join(t[i:i+n]) for i in range(len(t)-n+1)]\n",
    "        c.update(grams)\n",
    "    return pd.DataFrame(c.most_common(k), columns=[f'{n}-gram','count'])\n",
    "\n",
    "print(\"\\nTop Unigrams (gesamt):\")\n",
    "print(top_ngrams(df['sentence'], n=1, k=10))\n",
    "\n",
    "print(\"\\nTop Bigrams (gesamt):\")\n",
    "print(top_ngrams(df['sentence'], n=2, k=10))\n",
    "\n",
    "for lbl in sorted(df['label'].unique()):\n",
    "    subset = df.loc[df['label']==lbl, 'sentence']\n",
    "    print(f\"\\nTop Unigrams (Label={lbl}):\")\n",
    "    print(top_ngrams(subset, n=1, k=10))\n",
    "    print(f\"\\nTop Bigrams (Label={lbl}):\")\n",
    "    print(top_ngrams(subset, n=2, k=10))\n",
    "\n",
    "#Vokabulargr√∂√üe (ohne Stopwords)\n",
    "def vocab_size(texts):\n",
    "    v=set()\n",
    "    for s in texts: v.update([w for w in tok(s) if w not in ENGLISH_STOP_WORDS])\n",
    "    return len(v)\n",
    "\n",
    "print(\"\\nVokabular gesamt (ohne Stopwords):\", vocab_size(df['sentence']))\n",
    "print(\"Vokabular je Label (ohne Stopwords):\")\n",
    "print(df.groupby('label')['sentence'].apply(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aab7b9",
   "metadata": {},
   "source": [
    "Explore Data\n",
    "\n",
    "Reviews with a score of 4 and 5 were considered to be positive, and scores of 1 and 2 to be negative.\n",
    "\n",
    "Amazon [1000 rows x 2 columns]: contains reviews and scores for products sold on amazon.com in the cell phones and accessories category.\n",
    "\n",
    "IMDb [748 rows x 2 columns]: refers to the IMDb movie review sentiment dataset originally introduced by Maas et al. as a benchmark for sentiment analysis.\n",
    "\n",
    "Yelp [1000 rows x 2 columns]: refers to the dataset from the Yelp dataset challenge from which we extracted the restaurant reviews.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
